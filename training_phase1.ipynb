{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autoreload classes in case you change something in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"classes\")\n",
    "from SLI_loss1 import LossCompute\n",
    "from model import *\n",
    "from networks import *\n",
    "from tokenizer import *\n",
    "from data_loader import *\n",
    "from prototype import get_prototypes\n",
    "from SLImodel import SLIModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./github_logs_.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "df['log_message'] = df['log_message'].str.replace(\"\\<\\*\\>\", \" \")\n",
    "df['log_message'] = df['log_message'].str.replace(\"\\[STR\\]\", \" \")\n",
    "df['log_message'] = df['log_message'].str.replace(\"\\[NUM\\]\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./filtered_log_df.csv\")\n",
    "load = df['log_message'].values\n",
    "labels = df['log_level'].values\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all special characters\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "df['log_message'] =df['log_message'].apply(lambda x:' '.join(regex.sub('', x ).strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['log_level']=='trace'] = 'debug'     \n",
    "# df.loc[df['log_level']=='critical'] = 'error'\n",
    "# df.loc[df['log_level']=='exception'] = 'debug'\n",
    "# df.loc[df['log_level']=='fatal'] = 'error'\n",
    "# df.loc[df['log_level']=='warn'] = 'debug'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['log_level']=='trace'] = 'debug'     \n",
    "df.loc[df['log_level']=='critical'] = 'error'\n",
    "df.loc[df['log_level']=='exception'] = 'error'\n",
    "df.loc[df['log_level']=='fatal'] = 'error'\n",
    "df.loc[df['log_level']=='warn'] = 'warning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['log_level']!='debug']\n",
    "df = df[df['log_level']!='log']\n",
    "df = df[df['log_level']!='warning']\n",
    "df = df.reset_index()\n",
    "df = df.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df.log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn(x):\n",
    "    if x ==\"warning\":\n",
    "        return \"anomaly\"\n",
    "    elif x==\"error\":\n",
    "        return \"anomaly\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "df.log_level = df.log_level.apply(lambda x: fcn(x))\n",
    "# df[df.log_level=='warning'].log_level = 'normal'\n",
    "# df[df.log_level=='info'].log_level = 'normal'\n",
    "# df[df.log_level=='error'].log_level = 'anomaly'\n",
    "# normal class: info, warning\n",
    "# anomaly class: critical, error, exception, fatal,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:, [\"log_level\", \"log_message\"]]\n",
    "df1.columns = [\"t\", \"Content\"]\n",
    "df1 = df1[df1.t==\"anomaly\"].drop_duplicates()\n",
    "df1.Content = df1.Content.apply(lambda x: \" \".join([z.lower() for z in x.rsplit()]))\n",
    "df1.to_csv(\"anomalies_github.csv\", sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'debug': 0, 'error': 1, 'info': 2, 'log': 3, 'warning': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only unique log messages\n",
    "# df = df.drop_duplicates(subset=['log_message']).reset_index().drop(columns=['index'])\n",
    "\n",
    "load = df['log_message'].values\n",
    "labels = df['log_level'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class conunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = df.groupby(\"log_level\").count()['log_message']\n",
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {class_count.index[i]:i for i in range(len(class_count))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LogTokenizer()\n",
    "# tokenizer = LogTokenizer(\"tokenizer_SLI.json\")\n",
    "tokenized = []\n",
    "for i in trange(0, len(df)):\n",
    "        tokenized.append(np.array(tokenizer.tokenize(df['log_message'][i])))\n",
    "        \n",
    "labels_tokenized = [label_mapper[label] for label in labels]\n",
    "# labels_tokenized = pickle.load(open(\"tokenizer/tokenizer.pickle\",'rb'))\n",
    "# tokenizer = pickle.load(open(\"tokenizer_SLI.json\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer/tokenizer_256.pickle\",'wb') as file:\n",
    "    pickle.dump(tokenizer,file,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"tokenizer/labels_tokenized_256.pickle\",'wb') as file:\n",
    "    pickle.dump(labels_tokenized,file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train,load_test, labels_train, labels_test = train_test_split(np.array(tokenized), np.array(labels_tokenized),train_size=0.8)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "pad_len = 50\n",
    "train_dataloader, test_dataloader = create_data_loaders(load_train, labels_train, load_test,\n",
    "                                                                     labels_test, pad_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_vocab = tokenizer.n_words\n",
    "tgt_vocab = 256\n",
    "n_layers=2\n",
    "in_features=256\n",
    "out_features=256\n",
    "num_heads=2\n",
    "dropout=0.05\n",
    "max_len=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_weights = lambda x, i: x.sum() / (len(x)*x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [calculate_weights(class_count,i) for i in range(len(class_count))]\n",
    "weights /= max(weights)\n",
    "class_weights=torch.FloatTensor(weights).cuda()\n",
    "\n",
    "cross_entropoy_loss = nn.CrossEntropyLoss(weight=class_weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./weights_class3.pickle\", \"wb\") as file:\n",
    "    pm = pickle.dump(weights, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspherical Prototype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"classes\": 5,\n",
    "    \"dims\": 16,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": 1000,\n",
    "    \"seed\": 300,\n",
    "\n",
    "}\n",
    "prototypes = torch.from_numpy(get_prototypes(conf)).float()\n",
    "\n",
    "cos_sim = nn.CosineSimilarity(eps=1e-9).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototypes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(dataloader, model, optimizer, f_loss, epoch,polars=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "#     start = time.time()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        load, y = batch\n",
    "        if polars  is not None:\n",
    "            y = polars[y.numpy()]\n",
    "            y = torch.autograd.Variable(y).cuda()\n",
    "    \n",
    "        out = model.forward(load.cuda().long(), None)\n",
    "\n",
    "        if isinstance(f_loss,nn.CosineSimilarity):\n",
    "             loss = (1 - f_loss(out,y)).pow(2).sum()\n",
    "        else:\n",
    "            loss = f_loss(out,y.cuda().long())\n",
    "        \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "#         elapsed = time.time() - start\n",
    "        if i%5==0:\n",
    "            print(\"Epoch %d Train Step: %d / %d Loss: %f\" %\n",
    "                  (epoch,i, len(dataloader), loss), end='\\r')\n",
    "    \n",
    "    print(\"Epoch %d Train Step: %d / %d Loss: %f\" %\n",
    "                  (epoch,i, len(dataloader), loss), end='\\r')        \n",
    "    return total_loss/len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "def run_test(dataloader, model, optimizer, f_loss, epoch, polars=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds = []\n",
    "    tmps = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            load, y = batch\n",
    "\n",
    "\n",
    "            out = model.forward(load.cuda().long(), None)\n",
    "#             print(out)\n",
    "            if isinstance(f_loss,nn.CosineSimilarity):\n",
    "                x = F.normalize(out, p=2, dim=1)\n",
    "\n",
    "                x = torch.mm(x, polars.t().cuda())\n",
    "                pred = x.max(1, keepdim=True)[1].reshape(1,-1)[0]\n",
    "                preds += list(pred.detach().cpu().numpy())\n",
    "            else:\n",
    "                tmp = out.detach().cpu().numpy()\n",
    "                preds += list(np.argmax(tmp, axis=1))\n",
    "                tmps += list(tmp)\n",
    "#             if i%5==0:\n",
    "#                 print(\"Epoch %d Test Step: %d / %d Loss: %f\" %\n",
    "#                       (epoch,i, len(dataloader), loss), end='\\r')\n",
    "\n",
    "#     print(\"Epoch %d Test Step: %d / %d Loss: %f\" %\n",
    "#                   (epoch,i, len(dataloader), loss), end='\\r')        \n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "def run_optimizer(model, train_dataloader,labels_test,optimizer,n_epochs,f_loss,polars,class_weights):\n",
    "    conf_matrix = []\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    for epoch in range(1,1+n_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "        start_train_time = time()\n",
    "        loss = run_train(train_dataloader, model, optimizer, f_loss, epoch,polars)\n",
    "        train_time.append(time()-start_train_time)\n",
    "        print(\"Epoch %d Train Loss: %f\" % (epoch, loss),\" \"*30) \n",
    "        \n",
    "        start_test_time = time()\n",
    "        preds = run_test(test_dataloader, model, optimizer, f_loss, epoch,polars)\n",
    "        test_time.append(time()-start_test_time)\n",
    "#         print(\"Epoch %d Test Loss: %f\" % (epoch, loss),\" \"*30)\n",
    "        print(f\"Accuracy:{round(accuracy_score(preds,labels_test),2)}\")\n",
    "        print(f\"f1_score:{round(f1_score(preds,labels_test,average='weighted'),2)}\")\n",
    "        print(f\"recall_score:{round(recall_score(preds,labels_test,average='weighted'),2)}\")\n",
    "        print(f\"precision_score:{round(precision_score(preds,labels_test,average='weighted'),2)}\")\n",
    "        conf_matrix.append(confusion_matrix(preds,labels_test))\n",
    "    return model, preds, conf_matrix, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers['adam']\n",
    "n_epochs = 20\n",
    "loss_f = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SLIModel(src_vocab=src_vocab, tgt_vocab=2,\n",
    "                     n_layers=n_layers, in_features=in_features,\n",
    "                     out_features=out_features,num_heads=num_heads,\n",
    "                     dropout=dropout, max_len=max_len).get_model()\n",
    "torch.cuda.set_device(0)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.0001\n",
    "decay = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "momentum = 0.9\n",
    "sgd_opt = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=decay)\n",
    "adam_opt = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizers = {\"adam\":adam_opt,\"sgd\":sgd_opt}\n",
    "optimizer = optimizers['adam']\n",
    "n_epochs = 50\n",
    "loss_f = cross_entropoy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, preds, conf_matrix, train_time, test_time = run_optimizer(model,train_dataloader,labels_test,optimizer,n_epochs,cross_entropoy_loss,polars=None,class_weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path= \"./model_params_2classes_prototypes.pickle\"\n",
    "tokenizer_path = \"./tokenizer_dict_model_params_model_params_2classes_prototypes.json\"\n",
    "\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "state_dict.pop('generator.proj.bias', None)\n",
    "state_dict.pop('generator.proj.weight', None)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(model_weights_path,'wb') as file:\n",
    "    pickle.dump(state_dict,file,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open(tokenizer_path,'w') as file:\n",
    "    json.dump(tokenizer.word2index,file)\n",
    "\n",
    "with open(\"train_time256.pickle\", \"wb\") as file:\n",
    "    pickle.dump(train_time, file)\n",
    "\n",
    "with open(\"test_time256.pickle\", \"wb\") as file:\n",
    "    pickle.dump(test_time, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
