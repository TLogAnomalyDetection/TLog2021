{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # use only for init\n",
    "df= pd.DataFrame(columns=[\"f1_score\",\"recall\",\"precision\",\"accuracy\",\"confusion_matrix\"])\n",
    "output_file_name = \"bgl_exp.csv\"\n",
    "output_exp_2 = \"experiment_bgl_output_dry_run1.txt\"\n",
    "df.to_csv(output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "\n",
    "torch.random.seed = 0\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from tokenizer import LogTokenizer\n",
    "from utils import get_padded_data\n",
    "from classes.loss_functions import LossCompute\n",
    "from classes.model import TLogModel\n",
    "from trainer import run_train, run_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tlog import TLogRunner\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(dataset_name):\n",
    "    if dataset_name == \"bgl\":\n",
    "        bgl_dataset = BGLDataset()\n",
    "        every_n = 100\n",
    "        aux_size = 100000\n",
    "        spirit_dataset = SpiritDataset(every_n=every_n, aux_size=aux_size,aux=True)\n",
    "        intrepid_dataset = InterpidScrubbedDataset(aux=True)\n",
    "        max_lines = 5000000\n",
    "        tbird_dataset = ThunderbirdDataset(max_lines=max_lines,aux=True)\n",
    "        \n",
    "        dataset = bgl_dataset\n",
    "        aux_datasets = [tbird_dataset,spirit_dataset,intrepid_dataset]\n",
    "        \n",
    "    elif dataset_name == \"tbird\":\n",
    "        bgl_dataset = BGLDataset(aux=True)\n",
    "        every_n = 1\n",
    "        aux_size = 100000\n",
    "        spirit_dataset = SpiritDataset(every_n=every_n, aux_size=aux_size,aux=True)\n",
    "        intrepid_dataset = InterpidScrubbedDataset(aux=True)\n",
    "        max_lines = 5000000\n",
    "        tbird_dataset = ThunderbirdDataset(max_lines=max_lines)\n",
    "        \n",
    "        dataset = tbird_dataset\n",
    "        aux_datasets = [bgl_dataset,spirit_dataset,intrepid_dataset]\n",
    "        \n",
    "    elif dataset_name == \"spirit\":\n",
    "        bgl_dataset = BGLDataset(aux=True)\n",
    "        every_n = 100  # when spirit training use every_n=50\n",
    "        aux_size = 100000\n",
    "        spirit_dataset = SpiritDataset(every_n=every_n)\n",
    "        intrepid_dataset = InterpidScrubbedDataset(aux=True)\n",
    "        max_lines = 5000000\n",
    "        tbird_dataset = ThunderbirdDataset(max_lines=max_lines,aux=True)\n",
    "        \n",
    "        dataset = spirit_dataset\n",
    "        aux_datasets = [bgl_dataset,tbird_dataset,intrepid_dataset]\n",
    "        \n",
    "    elif dataset_name == \"interpid\":\n",
    "        bgl_dataset = BGLDataset(aux=True)\n",
    "        every_n = 100\n",
    "        aux_size = 100000\n",
    "        spirit_dataset = SpiritDataset(every_n=every_n,aux=True)\n",
    "        intrepid_dataset = InterpidScrubbedDataset()\n",
    "        max_lines = 5000000\n",
    "        tbird_dataset = ThunderbirdDataset(max_lines=max_lines,aux=True)\n",
    "        \n",
    "        dataset = intrepid_dataset\n",
    "        aux_datasets = [bgl_dataset,tbird_dataset,spirit_dataset]\n",
    "    return dataset, aux_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = []\n",
    "# for dataset_name in [\"bgl\",\"tbird\",\"spirit\"]:\n",
    "# for dataset_name in [\"bgl\", \"spirit\", \"tbird\"]:\n",
    "# for dataset_name in [\"bgl\"]:\n",
    "for dataset_name in [\"bgl\"]:\n",
    "#     for split_size in [0.001, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]:\n",
    "#     for split_size in [0.2, 0.4, 0.6, 0.8]:\n",
    "    for split_size in [0.8]:\n",
    "        for pretrain in [True]:\n",
    "            for tokenizer in [True]:\n",
    "                for use_generator in [True]:\n",
    "                    experiment_params.append((dataset_name, pretrain, tokenizer, split_size, use_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indecies_splits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = open(output_exp_2, 'a+')\n",
    "for i, experiment in enumerate(experiment_params):\n",
    "# for i, experiment in enumerate([('bgl', False, False, 0.2, False)]):\n",
    "    print(\"Experiment:\",i)\n",
    "    print(\"Experiment:\",i,file=fp)\n",
    "    dataset_name,pretrain,tokenizer,split_size,use_generator = experiment\n",
    "    print(\"---\"*10)\n",
    "    print(\"Running experiment with parameters:\")\n",
    "    print(\"Dataset:\",dataset_name)\n",
    "    print(\"Pretrain:\",pretrain)\n",
    "    print(\"Tokenizer:\",tokenizer)\n",
    "    print(\"Split size:\",split_size)\n",
    "    print(\"Generator:\",use_generator)\n",
    "    print(\"---\"*10,file=fp)\n",
    "    print(\"Running experiment with parameters:\",file=fp)\n",
    "    print(\"Dataset:\",dataset_name,file=fp)\n",
    "    print(\"Pretrain:\",pretrain,file=fp)\n",
    "    print(\"Tokenizer:\",tokenizer,file=fp)\n",
    "    print(\"Split size:\",split_size,file=fp)\n",
    "    print(\"Generator:\",use_generator,file=fp)\n",
    "    fp.flush()\n",
    "    dataset, aux_datasets = load_datasets(dataset_name)\n",
    "\n",
    "   \n",
    "    if pretrain:\n",
    "        model_weights_path= \"./model_params_2classes_prototypes.pickle\"\n",
    "    else:\n",
    "        model_weights_path = None\n",
    "\n",
    "    if tokenizer:\n",
    "        tokenizer_path = \"./tokenizer_dict_model_params_model_params_2classes_prototypes.json\"\n",
    "    else:\n",
    "        tokenizer_path = None\n",
    "\n",
    "\n",
    "    model_params = {\n",
    "        \"tgt_vocab\":16,\n",
    "        \"n_layers\":2,\n",
    "        \"in_features\":16,\n",
    "        \"out_features\":16,\n",
    "        \"num_heads\":2,\n",
    "        \"dropout\":0.05,\n",
    "        \"max_len\":50\n",
    "    }\n",
    "\n",
    "    \n",
    "    if experiment[1] == False:\n",
    "        batch_size=2048\n",
    "        epochs=30\n",
    "    else:\n",
    "        if dataset_name==\"spirit\":\n",
    "            if split_size == 0.001:\n",
    "                batch_size=2048\n",
    "            if split_size == 0.05:\n",
    "                batch_size=1024\n",
    "            if split_size == 0.2:\n",
    "                batch_size=256\n",
    "            if split_size == 0.4:\n",
    "                batch_size=128\n",
    "            if split_size == 0.6:\n",
    "                batch_size=64\n",
    "            if split_size == 0.8:\n",
    "                batch_size=64\n",
    "        batch_size=256\n",
    "        epochs=3\n",
    "    \n",
    "    pad_len=50\n",
    "    seed=0\n",
    "    loss_criterion = nn.CrossEntropyLoss()#\n",
    "   \n",
    "    model_path='./models/'\n",
    "\n",
    "\n",
    "    tlog_run = TLogRunner(dataset=dataset,\n",
    "                       aux_datasets=aux_datasets,\n",
    "                       model_params=model_params,\n",
    "                       seed=seed,\n",
    "                       split_size=split_size,\n",
    "                       batch_size=batch_size,\n",
    "                       pad_len=pad_len,\n",
    "                       loss_criterion=loss_criterion,\n",
    "                       model_path=model_path,\n",
    "                       epochs=epochs,\n",
    "                       model_weights_path=model_weights_path,\n",
    "                       tokenizer_path=tokenizer_path,\n",
    "                       use_generator=use_generator)\n",
    "\n",
    "    load, labels = tlog_run.init_data(dataset, aux_datasets)\n",
    "    log_payload, labels = tlog_run.tokenize_data(load,labels)\n",
    "    \n",
    "#     labels = np.where(labels==0, 1, 0)\n",
    "    \n",
    "    load_train, labels_train, load_test, labels_test = tlog_run.train_test_split(log_payload, labels.flatten())\n",
    "    train_dataloader, test_dataloader = tlog_run.create_data_loaders(load_train, labels_train, load_test,\n",
    "                                                                 labels_test)\n",
    "    indecies_splits[str(split_size)] = train_dataloader.dataset.tensors[0].numpy().shape[0]\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "    \n",
    "    un, class_count = np.unique(labels_train, return_counts=True)\n",
    "    \n",
    "    print(class_count)\n",
    "    calculate_weights = lambda x, i: x.sum() / (len(x)*x[i])\n",
    "    weights = [calculate_weights(class_count,i) for i in range(len(class_count))]\n",
    "    weights /= max(weights)\n",
    "\n",
    "    class_weights=torch.FloatTensor(weights).cuda()\n",
    "    \n",
    "    if pretrain == False:\n",
    "        class_weights=torch.FloatTensor(torch.tensor([0.3, 1.0])).cuda()\n",
    "    \n",
    "    tlog_run.loss_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    tlog_run.run_optimizer(train_dataloader, test_dataloader, log_payload, labels_test)\n",
    "    max_f1=0\n",
    "    best_res=None\n",
    "    thresholds = []\n",
    "    flag = True\n",
    "    \n",
    "    dataset_name = experiment[0]\n",
    "    \n",
    "    path = \"./log_anomaly_detection_evaluation/\" + dataset_name + \"/\"\n",
    "    model_name = \"tlog_\"\n",
    "    phase = \"prediction_\"\n",
    "    with open (path + model_name + phase + \"_\".join([str(x) for x in experiment]) + \".pickle\", \"wb\") as file:\n",
    "        pickle.dump(tlog_run.max_distances, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
